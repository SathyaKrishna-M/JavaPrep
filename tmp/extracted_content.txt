

--- START OF Session-1_7829 ALI.pdf ---

 
 
 
Course Name: Mathematics for AI 
Explain course handout:  
Session-1: Understand vector representations and similarity measures 
Vectors 
â€¢ Definition of a vector in â„ğ‘› 
A vector in â„ğ‘›is an ordered ğ‘›-tuple of real numbers. 
ğ± = (ğ‘¥1, ğ‘¥2, ğ‘¥3, â€¦ , ğ‘¥ğ‘›),where ğ‘¥ğ‘– âˆˆ â„ 
Here: 
â€¢ ğ‘›is a positive integer called the dimension, 
â€¢ each ğ‘¥ğ‘–is called a component (or coordinate) of the vector. 
Set-Theoretic Description 
â„ğ‘› = {(ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘›) âˆ£ ğ‘¥ğ‘– âˆˆ â„} 
Geometric Interpretation 
â€¢ In â„2: a vector represents a point or directed line segment in a plane. 
â€¢ In â„3: a vector represents a point or directed line segment in space. 
â€¢ For ğ‘› > 3: vectors exist in higher-dimensional spaces (cannot be visualized directly 
but handled algebraically). 
 
Algebraic Interpretation 
A vector in â„ğ‘›is an element of a real vector space equipped with: 
â€¢ vector addition 
â€¢ scalar multiplication 
Example-1: u = (2,4) âˆˆ â„2 ğšğ§ğ ğ¯ = (2, âˆ’1,3) âˆˆ â„3. 
Notation 
â€¢ Row vector: (ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘›) 
â€¢ Column vector: 
(
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
)  

Vector addition in â„ğ’: Let ğ‘¢ = (ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘›) and v= (ğ‘¦1, ğ‘¦2, â€¦ , ğ‘¦ğ‘›) be two vectors in â„ğ’ 
then, their sum ğ‘¢ + ğ‘£ is defined as:  
ğ‘¢ + ğ‘£ = (ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘›) + (ğ‘¦1, ğ‘¦2, â€¦ , ğ‘¦ğ‘›) = (ğ‘¥1 + ğ‘¦1, ğ‘¥2 + ğ‘¦2, â€¦ , ğ‘¥ğ‘› + ğ‘¦ğ‘›). 
Example 2: Let ğ’—ğŸ = (2, âˆ’1,3) âˆˆ â„3 and ğ’—ğŸ = (3, âˆ’4,6) âˆˆ â„3 then Let ğ’—ğŸ + ğ’—ğŸ =
(5, âˆ’5,9) âˆˆ â„3. 
Example 3: A drone flies 3 km east and then 4 km north. Find the resultant displacement. 
Example 4: Three forces 5 N along x-axis, â€“3 N along y-axis, and 4 N along z-axis act on a 
particle. Find the resultant force. 
Dot Product 
u â‹… ğ‘£ = âˆ‘ ğ‘¥ğ‘–
ğ‘›
ğ‘–=1
ğ‘¦ğ‘– 
Problem 1: Let u = (1, âˆ’2,3,4), v = (2,0, âˆ’1,5). Compute u â‹… v. 
Problem 2: A force ğ¹âƒ— = (3, âˆ’2,6)â€‰N moves a body through a displacement ğ‘ âƒ— = (4,1,2)â€‰m. 
Compute the work done. 
Properties:  
(i) Commutative: ğ® â‹… ğ¯ = ğ¯ â‹… ğ® for all ğ®, ğ¯ âˆˆ â„ğ‘›. 
(ii) Distributive: (ğ® + ğ¯) â‹… ğ° = ğ® â‹… ğ° + ğ¯ â‹… ğ°. 
Geometric meaning: ğ‘¢ â‹… ğ¯ =âˆ¥ ğ® âˆ¥ .â€‰ âˆ¥ ğ¯ âˆ¥ cos ğœƒ 
 
Vector Norms: Euclidean norm: âˆ¥ ğ¯ âˆ¥= âˆšâˆ‘ğ‘¦ğ‘–
2  
Meaning of magnitude and length: Distance between vectors 
Problem-1: Find the Euclidean norm of ğ¯ = (1, â€Š âˆ’ 2, â€Š2, â€Š1). 
Problem-2: Let ğ’— = (ğ‘¥, â€Š2ğ‘¥, â€Š3ğ‘¥), Find âˆ¥ ğ’— âˆ¥in terms of ğ‘¥. 
 
Projection of One Vector onto Another: The projection of a vector u onto another vector v 
is the vector component of uthat lies in the direction of v. It is given by 

 
Problem-1: Find the projection of u = (3,4) onto v = (1,0). 
Problem-2: Show that the vector u âˆ’ proj v(u) is orthogonal to v. 
Problem-3: Let force ğ… = (5,3,4) acts on a body which moves along the direction ğ =
(1,2,2). Find the component of the force along the direction of motion. 
Session-2: Represent AI data using vectors 
Representing Vectors Digitally 
A vector as a list/array: x = [ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘›] 
 
â€¢ Concept of dimension 
â€¢ High-dimensional vectors (motivation for text data) 
Example 1: Three feature embeddings are ğ‘’âƒ—1 = (1,2,3), ğ‘’âƒ—2 = (2, âˆ’1,1), ğ‘’âƒ—3 = (âˆ’1,1,2). 
Find the combined embedding vector. 
Solution:  ğ‘…âƒ—âƒ— = ğ‘’âƒ—1 + ğ‘’âƒ—2 + ğ‘’âƒ—3 
= (1 + 2 âˆ’ 1)ğ‘–Ì‚+ (2 âˆ’ 1 + 1)ğ‘—Ì‚+ (3 + 1 + 2)ğ‘˜Ì‚ = (2,â€‰2,â€‰6) 
Magnitude: âˆ£ ğ‘…âƒ—âƒ— âˆ£= âˆš4 + 4 + 36 = âˆš44 â‰ˆ 6.63 
Answer: Combined embedding = (2, 2, 6). 
Problem 2: Let u = (ğ‘¥, 2ğ‘¥, 3ğ‘¥, â€¦ , ğ‘›ğ‘¥), v = (1,1,1, â€¦ ,1). Find u â‹… v. 
Problem 3: Wind velocity is ğ‘£âƒ— = (6,8,2)â€‰m/s. The runway direction is given by the unit 
vector ğ‘›Ì‚ = (
3
5
, 4
5
, 0). Find the wind component along the runway. 
Problem-4: A feature vector in an ML model is ğ± = (4, â€Š3, â€Š12), Compute its Euclidean norm. 
Problem-5: If âˆ¥ ğ¯ âˆ¥= 5 and ğ¯ = (ğ‘, â€Šğ‘, â€Šğ‘). find all possible values of ğ‘, ğ‘, ğ‘ satisfying the 
condition. 
Cosine Similarity (Core Concept) 
Definition: Cosine Similarity(a, b) =
aâ‹…b
âˆ¥aâˆ¥â€‰âˆ¥bâˆ¥ 
Why cosine? 
â€¢ Independent of magnitude 
â€¢ Measures directional similarity 
â€¢ Values lie in [âˆ’1,1] 

Value Meaning 
1 Identical direction 
0 Orthogonal (unrelated) 
âˆ’1 Opposite direction 
 
Case Study 1: Movie Recommendation Based on Plot Similarity             
Problem 
A streaming platform wants to recommend movies with similar stories based on their 
descriptions. 
Movie Description 
D1 â€œA young hero saves the world from aliensâ€ 
D2 â€œA brave hero fights aliens to save Earthâ€ 
D3 â€œA romantic story about love and friendshipâ€ 
Step 1: Text Preprocessing 
We first clean the text: 
â€¢ Convert to lowercase 
â€¢ Remove stop words (a, the, to, from, etc.) 
â€¢ Tokenize words 
Key words extracted 
â€¢ D1: {young, hero, saves, world, aliens} 
â€¢ D2: {brave, hero, fights, aliens, save, earth} 
â€¢ D3: {romantic, story, love, friendship} 
Step 2: Vocabulary Construction 
Unique words across all descriptions: 
[young, hero, saves, world, aliens, brave, fights, earth, romantic, story, love, friendship] 
Step 3: Vector Representation (Bag of Words) 
Each movie is represented as a vector based on word presence (1 = present, 0 = absent). 
Word â†“ 
/ Movie 
â†’ 
young hero saves world aliens brave fights earth romantic story love friendship 
D1 1 1 1 1 1 0 0 0 0 0 0 0 
D2 0 1 0 0 1 1 1 1 0 0 0 0 
D3 0 0 0 0 0 0 0 0 1 1 1 1 
 Step 4: Similarity Measurement (Cosine Similarity) 
Cosine Similarity(ğ´, ğµ) = ğ´ â‹… ğµ
âˆ¥ ğ´ âˆ¥âˆ¥ ğµ âˆ¥ 
D1. D2 = 2, ||ğ·1|| = âˆš5, ||ğ·2|| = âˆš5,  
Cosine Similarity(ğ·1, ğ·2) = 2
âˆš5. âˆš5
= 2
5 = 0.4 
Similarity Scores 
Similarity (D1, D2) â‰ˆ 0.4 (High similarity), Similarity (D1, D3) = 0 and Similarity (D2, D3) = 0 
Step 5: Recommendation Logic 
â€¢ D1 and D2 share common themes: hero + aliens + saving Earth 
â€¢ D3 is romance-based, unrelated to the first two 
 
Practical-1: News Article Similarity Analysis Using TF-IDF 
Step 1: Converting Text into Vectors 
Method: TFâ€“IDF (Frequency-Based Representation) 
1. Words are treated as dimensions 
2. Each document becomes a vector: 
ğ = (ğ‘¤1, ğ‘¤2, â€¦ , ğ‘¤ğ‘›) 
TF (Term Frequency): 
TF(ğ‘¡, ğ‘‘) = Number of times term ğ‘¡ appears in ğ‘‘
Total terms in ğ‘‘  
IDF (Inverse Document Frequency):IDF(ğ‘¡) = log (
ğ‘
DF(ğ‘¡)) 
TF-IDF: TF-IDF(ğ‘¡, ğ‘‘) = TF Ã— IDF 
 
Interpretation (Very Important for Exams & AI Context) 
â€¢ The similarity is zero because the two documents share no common weighted terms 
after TFâ€“IDF. 
â€¢ Even though both documents contain â€œAIâ€, its TFâ€“IDF weight is zero, since it 
appears in all documents. 
â€¢ Hence, TFâ€“IDF correctly identifies that the documents are conceptually related but 
lexically different. 
Step 2: Measuring Similarity 
Once documents are vectors: 
Similarity = ğ1 â‹… ğ2
âˆ¥ ğ1 âˆ¥ â€‰ âˆ¥ ğ2 âˆ¥ 
Example:1 
Document 1: â€œMathematics is vital for AIâ€ 
Document 2: â€œAI depends on mathematical modelsâ€ 
Given Documents 
â€¢ Document 1 (Dâ‚): â€œMathematics is vital for AIâ€ 
â€¢ Document 2 (Dâ‚‚): â€œAI depends on mathematical modelsâ€ 
Step 1: Text Preprocessing 
(a) Convert to lowercase 
(b) Remove punctuation 
(c) Tokenization 
(d) Remove stop words (like is, for, on) 
 
 
Processed Documents 
Dâ‚: mathematics, vital, ai  
Dâ‚‚: ai, depends, mathematical, models 
Step 2: Vocabulary Construction 
List all unique terms from both documents. 
ğ‘‰ = {mathematics,vital,ai,depends,mathematical,models} 
Total documents: ğ‘ = 2 
Step 3: Term Frequency (TF) 
TF(ğ‘¡, ğ‘‘) = Number of times term ğ‘¡ appears in ğ‘‘
Total terms in ğ‘‘  
TF for Document 1 
Total words in Dâ‚ = 3 
Term mathematics vital ai depends mathematical models 
Frequency 1 1 1 0 0 0 
TF 
      
0 0 0 
TF for Document 2 
Total words in Dâ‚‚ = 4 
Term ai depends mathematical models mathematics vital 
Frequency 1 1 1 1 0 0 
         TF 
        
0 0 
 
Step 4: Document Frequency (DF) 
DF(ğ‘¡) = Number of documents containing term ğ‘¡ 
 
Term mathematics vital ai depends mathematical models 
DF 1 1 2 1 1 1 
Step 5: Inverse Document Frequency (IDF) 
IDF(ğ‘¡) = log ( ğ‘
DF(ğ‘¡)) 
Using natural logarithm (ln): 
IDF Values 

Term IDF 
mathematics (ğ¥ğ¨ğ (
ğŸ
ğŸ) = 0.693) 
vital (ğ¥ğ¨ğ (
ğŸ
ğŸ) = 0.693) 
ai (ğ¥ğ¨ğ (
ğŸ
ğŸ) = 0) 
depends (ğ¥ğ¨ğ (
ğŸ
ğŸ)= 0.693) 
mathematical (ğ¥ğ¨ğ (
ğŸ
ğŸ) = 0.693) 
models ((ğ¥ğ¨ğ (
ğŸ
ğŸ) = 0.693) 
  
Step 6: TFâ€“IDF Calculation 
TFâ€“IDF(ğ‘¡, ğ‘‘) = TF(ğ‘¡, ğ‘‘) Ã— IDF(ğ‘¡) 
TFâ€“IDF for Document 1: 
Term TF IDF TFâ€“IDF 
mathematics 1
3 0.693 0.231 
vital 1
3 0.693 0.231 
ai 1
3 0 0 
depends 0 0.693 0 
mathematical 0 0.693 0 
models 0 0.693 0 
 TFâ€“IDF for Document 2 
Term TF IDF TFâ€“IDF 
ai 1
4 0 0 
depends 1
4 0.693 0.173 
Term TF IDF TFâ€“IDF 
mathematical 1
4 0.693 0.173 
models 1
4 0.693 0.173 
mathematics   0 0.693 0 
vital   0 0.693 0 
  
Step 7: Final TFâ€“IDF Vectors 
Document 1 Vector 
ğ·1 = (0.231, â€Š0.231, â€Š0, â€Š0, â€Š0, â€Š0) 
 
Document 2 Vector 
ğ·2 = (0, â€Š0, â€Š0, â€Š0.173, â€Š0.173, â€Š0.173) 
Cosine Similarity Formula cos(ğœƒ) =
ğ·1â‹…ğ·2
||ğ·1||.||ğ·2|| 
As ğ·1. ğ·2 = 0. 
Therefore,                                           cos(ğœƒ) = 0 
Conclusion:  
What Cosine Similarity Actually Measures 
Cosine similarity measures the angle between two vectors, not their length. 
Cosine Similarity = cos (ğœƒ) 
â€¢ ğœƒ= angle between document vectors 
â€¢ It checks directional similarity, not size 
Geometric Interpretation (Very Important) 
Think of each document as a point/vector in high-dimensional space. 
Case 1: Cosine similarity = 1 
ğœƒ = 0âˆ˜ 
 
â€¢ Vectors point in the same direction 
â€¢ Documents are identical in content distribution 
â€¢ Same important words with similar weights 
Meaning: Documents are almost the same 
Case 2: Cosine similarity between 0 and 1 
0âˆ˜ < ğœƒ < 90âˆ˜ 
 
â€¢ Some overlap in important terms 
â€¢ Partial similarity 
Meaning: Documents are related to some extent 
Case 3: Cosine similarity = 0 
ğœƒ = 90âˆ˜ 
 
â€¢ Vectors are orthogonal 
â€¢ No overlapping important features 
Meaning: 
Documents are not similar in meaningful terms 
Very Important Clarification 
Turnitin / iThenticate similarity % â‰  Cosine similarity 
They do NOT use TFâ€“IDF cosine similarity the way we compute in NLP textbooks. 
Cosine similarity â†’ vector-based mathematical similarity 
 Turnitin/iThenticate â†’ text overlap / matching similarity 
Then what is the purpose of cosine similarity? 
Main Uses of Cosine Similarity (Very Important) 
Information Retrieval (Search Engines) 
When you search: 
â€œAI mathematical modelsâ€ 
The search engine: 
â€¢ Converts query + documents into vectors 
â€¢ Computes cosine similarity 
â€¢ Ranks documents by highest cosine similarity 
 Without cosine similarity, Google would not work. 
 
 


--- END OF Session-1_7829 ALI.pdf ---



--- START OF Session-2.pdf ---

 
 
 
Course Name: Mathematics for AI 
Week-2: Matrix operations, matrix multiplication and transformations  
 
Matrices: Matrices play a central role in linear algebra. They can be used to compactly  
represent systems of linear equations, but they also represent linear functions. 
Definition: Let ğ’, ğ’ âˆˆ â„•. A real -valued (ğ’, ğ’)-matrix ğ‘¨ is an ğ’ Ã— ğ’ tuple of elements  
ğ’‚ğ’Šğ’‹ âˆˆ  ğ‘…, where ğ’Š = ğŸ, ğŸ, â€¦ , ğ’ and ğ’‹ = ğŸ, ğŸ, â€¦ , ğ’, arranged according to a rectangular 
scheme consisting of ğ’ rows and ğ’ columns. 
[
ğ‘11 ğ‘12 â‹¯ ğ‘1ğ‘›
ğ‘21 ğ‘22 â‹¯ ğ‘2ğ‘›
â‹® â‹® â‹® â‹®
ğ‘ğ‘š1 ğ‘ğ‘š2 â€¦ ğ‘ğ‘šğ‘›
] 
By convention (1, n)-matrices are called rows and (m,1)-matrices are called row columns.  
These special matrices are also called row/column vectors. 
Definition (Matrix Addition): Let ğ´ = (ğ‘ğ‘–ğ‘—)and ğµ = (ğ‘ğ‘–ğ‘—)be two real-valued matrices of the 
same order ğ‘š Ã— ğ‘›. The sum of ğ´and ğµ, denoted by ğ´ + ğµ, is the matrix ğ¶ = (ğ‘ğ‘–ğ‘—)of order 
ğ‘š Ã— ğ‘›defined by ğ‘ğ‘–ğ‘— = ğ‘ğ‘–ğ‘— + ğ‘ğ‘–ğ‘—,for all ğ‘– = 1,2, â€¦ , ğ‘š and ğ‘— = 1,2, â€¦ , ğ‘›. 
That is, 
ğ´ + ğµ = [
ğ‘11 + ğ‘11 ğ‘12 + ğ‘12 â‹¯ ğ‘1ğ‘› + ğ‘1ğ‘›
ğ‘21 + ğ‘21 ğ‘22 + ğ‘22 â‹¯ ğ‘2ğ‘› + ğ‘2ğ‘›
â‹® â‹® â‹± â‹®
ğ‘ğ‘š1 + ğ‘ğ‘š1 ğ‘ğ‘š2 + ğ‘ğ‘š2 â‹¯ ğ‘ğ‘šğ‘› + ğ‘ğ‘šğ‘›
]. 
Note: Matrix addition is defined only when both matrices have the same dimensions. 
A. Practise Problems 
1. Let ğ´ = [2 3
1 4] , ğµ = [5 1
0 âˆ’2]. Find ğ´ + ğµ. 
2. If  ğ´ = [7 âˆ’3 2], ğµ = [âˆ’4 6 5], find ğ´ + ğµ. 
3. Let ğ´ = [
1
4
âˆ’2
] , ğµ = [
3
âˆ’1
5
]. Compute ğ´ + ğµ. 
4. If ğ´ = [1 2 3
4 5 6] , ğµ = [âˆ’1 0 2
3 âˆ’5 1], find ğ´ + ğµ. 
 

5. Given ğ´ = [ğ‘ ğ‘
ğ‘ ğ‘‘] , ğµ = [2ğ‘ âˆ’ğ‘
ğ‘ 3ğ‘‘ ], express ğ´ + ğµ. 
6. If ğ´ = [ğ‘¥ ğ‘¦
ğ‘§ ğ‘¤] , ğµ = [1 2
3 4], find ğ´ + ğµ when ğ‘¥ = 2, ğ‘¦ = âˆ’1, ğ‘§ = 0, ğ‘¤ = 3. 
Definition (Matrix Multiplication): 
Let ğ´ = (ğ‘ğ‘–ğ‘—)be a matrix of order ğ‘š Ã— ğ‘› and ğµ = (ğ‘ğ‘—ğ‘˜)be a matrix of order ğ‘› Ã— ğ‘. 
The product of ğ´and ğµ, denoted by ğ´ğµ, is the matrix ğ¶ = (ğ‘ğ‘–ğ‘˜)of order ğ‘š Ã— ğ‘, where each 
entry is defined by 
ğ‘ğ‘–ğ‘˜ = âˆ‘ ğ‘ğ‘–ğ‘—
ğ‘›
ğ‘—=1
â€‰ğ‘ğ‘—ğ‘˜,for ğ‘– = 1,2, â€¦ , ğ‘š and ğ‘˜ = 1,2, â€¦ , ğ‘. 
That is, the element in the ğ‘–-th row and ğ‘˜-th column of ğ´ğµis obtained by multiplying 
corresponding entries of the ğ‘–-th row of ğ´and the ğ‘˜-th column of ğµ, and then adding the 
products. 
Important Notes: 
1. Matrix multiplication is defined only when the number of columns of ğ´equals the 
number of rows of ğµ. 
2. In general, matrix multiplication is not commutative, i.e., ğ´ğµ â‰  ğµğ´. 
3. The order of the product ğ´ğµ is ğ‘š Ã— ğ‘. 
Practice Problems: Matrix Multiplication 
1. Compute ğ´ğµ, where ğ´ = [1 2
3 4] , ğµ = [2 0
1 5]. 
2. Find the product ğ´ğµ, where  ğ´ = [3 âˆ’1 2], ğµ = [
1
4
âˆ’2
]. 
3. If  ğ´ = [ 1 0
âˆ’2 3], compute ğ´2 = ğ´ â‹… ğ´. 
4. Determine whether the following products are defined. If yes, state the order of the 
result. 
(a) (2 Ã— 3)(3 Ã— 4)    (b) (3 Ã— 2)(3 Ã— 2)     (c) (4 Ã— 1)(1 Ã— 5) 
5. Let ğ´ be a 3 Ã— 2 matrix and ğµ be a 2 Ã— 4 matrix. What is the order of ğ´ğµ? 
6. Show by example that matrix multiplication is not commutative, i.e., ğ´ğµ â‰  ğµğ´. 
7. If  ğ´ğµ = 0, does it necessarily imply that ğ´ = 0or ğµ = 0? Explain with a 
counterexample. 
8. If ğ¼ is the identity matrix, show that ğ´ğ¼ = ğ¼ğ´ = ğ´. 
 
Geometric Interpretation of Matrix Multiplication 
Matrix multiplication can be understood geometrically as a linear transformation of space. 
1. Matrices as Transformations 
An ğ‘› Ã— ğ‘› matrix represents a linear transformation of â„ğ‘›: 
â€¢ It maps vectors to vectors, 
â€¢ It preserves straight lines and the origin, 
â€¢ It can stretch, shrink, rotate, reflect, or shear space. 
If ğ´ is a matrix and x is a vector, then ğ´x is the image of x after the transformation defined by 
ğ´. 
2. Column Interpretation:  
For a matrix ğ´ = [
âˆ£ âˆ£
a1 a2
âˆ£ âˆ£
], the columns a1, a2 are the images of the standard basis vectors: 
ğ´e1 = a1, ğ´e2 = a2. Thus, the matrix tells us where the coordinate axes go. 
3. Meaning of the Product ğ´ğµ 
Let ğ´ and ğµ be square matrices. The product ğ´ğµ represents composition of transformations: 
ğ´ğµx = ğ´(ğµx). 
Geometric meaning: 
1. First apply the transformation ğµ to vector x, 
2. Then apply the transformation ğ´ to the result. 
2D Example 
Consider ğ´ = [2 0
0 1] and ğµ = [0 âˆ’1
1 0 ]. 
â€¢ ğµ: rotation by 90âˆ˜counterclockwise, 
â€¢ ğ´: stretches the ğ‘¥-direction by a factor of 2. 
â€¢ ğ´ğµ: rotate first, then stretch, 
â€¢ ğµğ´: stretch first, then rotate. 
The final images are different, showing ğ´ğµ â‰  ğµğ´. 
Area and Volume Interpretation 
â€¢ The absolute value of the determinant of a matrix gives the area (in 2D) or volume (in 
3D) scaling factor. 
â€¢ For products: det (ğ´ğµ) = det (ğ´)det (ğµ), meaning the total scaling equals the 
product of individual scaling. 
Practise Problems:  
1. A map uses a linear transformation represented by ğ´ = [3 0
0 2] to enlarge regions. 
If the original region has area 5 square units, find the area after transformation. 
2. A square of area 10 square units undergo the transformation ğ´ = [1 ğ‘˜
0 1]. Find the 
area of the transformed square and comment on the role of ğ‘˜. 
3. A triangular region in the plane is transformed by ğ´ = [âˆ’1 0
0 1]. If the original area is 
8 square units: 
(a) find the area after transformation, (b) explain the sign of the determinant. 
5. Rotation Matrix 
A rotation in the plane is represented by ğ´ = [cos ğœƒ âˆ’sin ğœƒ
sin ğœƒ cos ğœƒ ]. Explain why the area 
of any region remains unchanged after rotation. 
Neural-Networkâ€“Style Geometric Interpretation (Linear Layers) 
In neural networks, each linear layer is a matrix multiplication, and geometry provides the 
intuition for how data is transformed layer by layer. 
1. Linear Layer as a Geometric Transformation 
A typical linear layer is written as y = ğ‘Šx + b, 
where 
â€¢ x âˆˆ â„ğ‘›= input vector, 
â€¢ ğ‘Š âˆˆ â„ğ‘šÃ—ğ‘›= weight matrix, 
â€¢ b âˆˆ â„ğ‘š= bias vector. 
Geometric meaning: 
â€¢ ğ‘Šxâ†’ linear transformation (stretching, rotating, shearing, projecting), 
â€¢ +bâ†’ translation of space. 
So, a linear layer reshapes the space and then shifts it. 
2. Column View (Very Important in NN) 
Let ğ‘Š = [
âˆ£ âˆ£ âˆ£
w1 w2 â‹¯ wğ‘›
âˆ£ âˆ£ âˆ£
]. Then  ğ‘Šx = ğ‘¥1w1 + ğ‘¥2w2 + â‹¯ + ğ‘¥ğ‘›wğ‘›. 
Interpretation: 
â€¢ Each column wğ‘— is a direction in output space, 
â€¢ Input coordinates ğ‘¥ğ‘— decide how much we move along each direction. 
Conclusion: A linear layer re-expresses the input vector in a new coordinate system. 
3. One Linear Layer (2D â†’ 2D Example) 
Suppose input points lie in the plane â„2. Let ğ‘Š = [2 1
0 1]. 
Geometric effect: 
â€¢ The unit square becomes a parallelogram,  
 
 
 
 
 
 
 
â€¢ Axes are skewed, 
 
 
 
 
 
â€¢ Relative positions of points are preserved (linearity). 
All input points are simultaneously transformed in the same way. 
4. Multiple Linear Layers = Composition of Transformations 
Consider two linear layers: h = ğ‘Š1x, y = ğ‘Š2h. Then y = (ğ‘Š2ğ‘Š1)x. 
Geometric meaning: 
â€¢ First layer ğ‘Š1: reshapes input space, 
â€¢ Second layer ğ‘Š2: reshapes the new space again, 
â€¢ The product ğ‘Š2ğ‘Š1is a single combined transformation. 
Conclusion: This is exactly matrix multiplication as composition of maps. 

5. Why Depth Without Nonlinearity Is Limited 
If a network has only linear layers: y = ğ‘Šğ‘˜ â‹¯ ğ‘Š2ğ‘Š1x, it is still just one linear transformation. 
Geometrically: 
â€¢ Multiple stretches/rotations collapse into one, 
â€¢ No bending or warping of space. 
Remark: Nonlinear activation functions (ReLU, tanh, sigmoid) are needed to: 
â€¢ Fold space, 
â€¢ Create curved decision boundaries, 
â€¢ Separate complex data. 
6. Geometric View of Classification 
â€¢ Linear layer â†’ maps data to a new space, 
â€¢ Hyperplanes become decision boundaries, 
â€¢ Training adjusts ğ‘Šso classes become linearly separable. 
In 2D: A line separates classes 
In higher dimensions: A hyperplane separates data clouds. 
Case Study: Predicting Student Performance Using a Neural Network Linear Layer 
Problem Context 
Suppose we want to predict a studentâ€™s final performance score based on three factors: 
1. Hours studied per week (ğ‘¥1) 
2. Attendance percentage (ğ‘¥2) 
3. Previous exam score (ğ‘¥3) 
We use a simple neural network with one linear layer having 2 neurons. 
Solution: Step 1: Input Vector (Student Data) For a particular student: 
      ğ‘‹ = [
ğ‘¥1
ğ‘¥2
ğ‘¥3
] = [
6
8
7
] 
Step 2: Weight Matrix of the Linear Layer 
Assume the network has learned the following weights: 
ğ‘Š = [0.5 0.2 0.3
0.1 0.6 0.4] 
â€¢ Neuron 1 focuses more on study hours 
â€¢ Neuron 2 focuses more on attendance and previous score 
Step 3: Bias Vector 
ğ‘ = [ 1
0.5] 
Step 4: Linear Transformation (Matrix Multiplication) ğ‘ = ğ‘Šğ‘‹ + ğ‘ 
First compute ğ‘Šğ‘‹: 
ğ‘Šğ‘‹ = [0.5 0.2 0.3
0.1 0.6 0.4] [
6
8
7
] 
Neuron 1 Output: (0.5)(6) + (0.2)(8) + (0.3)(7) = 3 + 1.6 + 2.1 = 6.7 
 
Neuron 2 Output: (0.1)(6) + (0.6)(8) + (0.4)(7) = 0.6 + 4.8 + 2.8 = 8.2 
 
Now add bias: ğ‘ = [6.7
8.2] + [ 1
0.5] = [7.7
8.7] 
Step 5: Interpretation of the Transformation 
â€¢ Input space: 3-dimensional (study, attendance, score) 
â€¢ Output space: 2-dimensional feature space 
â€¢ Matrix multiplication projects and reshapes the input into new features 
Remark: Each neuron computes a weighted combination of all inputs. 
Step 6: Activation Function (Optional) Apply ReLU (Rectified Linear Unit) activation: 
Mathematical Definition 
For any real number ğ‘§, ReLU(ğ‘§) = max (0, ğ‘§) 
That means: 
â€¢ If ğ‘§ > 0, output is ğ‘§ 
â€¢ If ğ‘§ â‰¤ 0, output is 0 
ğ´ = max (0, ğ‘) 
ğ´ = [7.7
8.7] 
ReLU is applied element by element to the vector ğ‘. That is, No change since values are 
positive. 
Step 7: Second Layer (Final Prediction) 
Assume a final layer to produce one performance score. 
Weight and Bias: ğ‘Š2 = [0.7 0.3], ğ‘2 = 0.5 
ğ‘Œ = ğ‘Š2ğ´ + ğ‘2 
ğ‘Œ = (0.7)(7.7) + (0.3)(8.7) + 0.5 
ğ‘Œ = 5.39 + 2.61 + 0.5 = 8.5 
Final Output:   Predicted Performance Score = 8. 
 
Problem-2: 
Suppose we want to predict a system performance index based on three input parameters: 
â€¢ ğ‘¥1: Input load level 
â€¢ ğ‘¥2: Resource utilization 
â€¢ ğ‘¥3: System response time 
We model this using a multi-layer neural network with: 
â€¢ Input layer: 3 neurons 
â€¢ Hidden layer 1: 2 neurons 
â€¢ Hidden layer 2: 2 neurons 
â€¢ Output layer: 1 neuron 
Solution: Step-1 Input Vector For a particular system: X = [
6
8
7
] 
Step 2: First Hidden Layer 
Weight Matrix and Bias ğ‘Š1 = [0.4 0.3 0.2
0.2 0.5 0.1] , ğ‘1 = [0.6
0.4] respectively. 
 

Linear Transformation 
ğ‘1 = ğ‘Š1ğ‘‹ + ğ‘1 
ğ‘1 = [(0.4)(4) + (0.3)(6) + (0.2)(5) + 0.6
(0.2)(4) + (0.5)(6) + (0.1)(5) + 0.4] = [5.0
4.7] 
Activation (ReLU) 
ğ´1 = max (0, ğ‘1) = [5.0
4.7]. 
Step 3: Second Hidden Layer 
Weight Matrix and Bias ğ‘Š2 = [0.6 0.2
0.3 0.7] , ğ‘2 = [0.3
0.2] 
Linear Transformation ğ‘2 = ğ‘Š2ğ´1 + ğ‘2 
ğ‘2 = [(0.6)(5.0) + (0.2)(4.7) + 0.3
(0.3)(5.0) + (0.7)(4.7) + 0.2] = [4.64
5.99] 
Activation (ReLU) 
ğ´2 = [4.64
5.99] 
Step 4: Output Layer 
Weight and Bias ğ‘Š3 = [0.5 0.5], ğ‘3 = 0.4 
 
Output Computation ğ‘Œ = ğ‘Š3ğ´2 + ğ‘3 
ğ‘Œ = (0.5)(4.64) + (0.5)(5.99) + 0.4 
ğ‘Œ = 2.32 + 2.995 + 0.4 = 5.715 
 
 
 



--- END OF Session-2.pdf ---



--- START OF Session-3-1.pdf ---

 
Course Name: Mathematics for AI 
Session- 3 Linearly Independent & Dependent Vectors: 
Definition: A set of vectors is linearly independent if the only way to write 
ğ‘1ğ‘£âƒ—1 + ğ‘2ğ‘£âƒ—2 + â‹¯ + ğ‘ğ‘›ğ‘£âƒ—ğ‘› = 0âƒ—âƒ— 
is when 
ğ‘1 = ğ‘2 = â‹¯ = ğ‘ğ‘› = 0. 
This means no vector in the set can be written as a combination of the others. 
Example: 
ğ‘£âƒ—1 = (1,0), ğ‘£âƒ—2 = (0,1) 
 
The equation          ğ‘1(1,0) + ğ‘2(0,1) = (0,0), gives  c1=0,c2=0 . 
So, these vectors are linearly independent. 
Definition: A set of vectors is linearly dependent if there exist constants, not all zero, such that 
ğ‘1ğ‘£âƒ—1 + ğ‘2ğ‘£âƒ—2 + â‹¯ + ğ‘ğ‘›ğ‘£âƒ—ğ‘› = 0âƒ—âƒ—. 
This means at least one vector can be written as a combination of the others. 
Example:ğ‘£âƒ—1 = (1,2), ğ‘£âƒ—2 = (2,4) 
 
Notice that 
ğ‘£âƒ—2 = 2ğ‘£âƒ—1 
So 
2ğ‘£âƒ—1 âˆ’ ğ‘£âƒ—2 = (0,0) 
Since the coefficients are not all zero, these vectors are linearly dependent. 
Example 1 (2D): vâƒ—âƒ—1 = (2,3), vâƒ—âƒ—2 = (1, âˆ’1) 
There is no scalar k  such that (2,3) = k(1, âˆ’1), so neither vector is a multiple of the other. 
Example 2 (3D): vâƒ—âƒ—1 = (1,0,0), vâƒ—âƒ—2 = (0,1,0), vâƒ—âƒ—3 = (0,0,1)  
These are the standard basis vectors in â„3. 
The only solution to c1vâƒ—âƒ—1 + c2vâƒ—âƒ—2 + c3vâƒ—âƒ—3 = (0,0,0)   is c1 = c2 = c3 = 0. 
Example 1 (2D):  vâƒ—âƒ—1 = (3,6), vâƒ—âƒ—2 = (âˆ’1, âˆ’2) 
Notice that  vâƒ—âƒ—1 = âˆ’3vâƒ—âƒ—2, So one vector is a scalar multiple of the other. 
Example 2 (3D): vâƒ—âƒ—1 = (1,2,3), vâƒ—âƒ—2 = (2,4,6), vâƒ—âƒ—3 = (1,0,1),  
Since                                           vâƒ—âƒ—2 = 2vâƒ—âƒ—1 
one vector depends on another, regardless of vâƒ—âƒ—3. 
 
 
 

Rank of a Matrix 
Definition: The rank of a matrix is the maximum number of linearly independent rows or 
columns of the matrix. 
Equivalently, 
â€¢ Rank = dimension of the column space 
â€¢ Rank = dimension of the row space 
It represents the amount of non-redundant information in the matrix. 
Example 1 (Solved) 
A = [1 2
2 4] 
 
Solution 
â€¢ Second row = 2 Ã— first row 
â€¢ Rows are linearly dependent 
rank(A) = 1  
 
Interpretation 
Only one independent direction exists â†’ redundancy present. 
Example 2 (Solved) 
B = [
1 0 2
0 1 3
0 0 0
] 
 
Solution 
â€¢ Two non-zero independent rows 
â€¢ Third row is zero 
rank(B) = 2  
 
Interpretation 
Matrix contains two independent features. 
â€¢ Basis of a Vector Space  
Definition: A basis of a vector space is a set of vectors that: 
1. Are linearly independent 
2. Span the vector space 
A basis is the minimum set of vectors needed to represent all vectors in the space. 
Example 1:       Consider vectors in â„2: 
v1 = [1
0] , v2 = [0
1] 
 
Solution 
â€¢ v1, v2are linearly independent 
â€¢ Any vector (x, y)can be written as 
xv1 + yv2 
{v1, v2} is a basis of â„2  
 
 
Example 2 : 
v1 = [
1
2
3
] , v2 = [
2
4
6
] , v3 = [
1
0
1
] 
Solution 
â€¢ v2 = 2v1â†’ dependent 
â€¢ v1and v3are independent 
{v1, v3} is a basis of the space spanned  
Orthogonal Vectors 
Definition: Two vectors are said to be orthogonal if their dot product is zero. 
v â‹… w = 0 
Orthogonal vectors represent independent, non-overlapping information. 
 
Example 1: 
ğ‘£ = [1
2] , ğ‘¤ = [ 2
âˆ’1] 
Solution 
ğ‘£ â‹… ğ‘¤ = (1)(2) + (2)(âˆ’1) = 2 âˆ’ 2 = 0 
ğ‘£ and ğ‘¤ are orthogonal  
Example 2: 
ğ‘£ = [
1
0
0
] , ğ‘¤ = [
0
3
0
] 
Solution 
ğ‘£ â‹… ğ‘¤ = 0 
ğ‘£ and ğ‘¤ are orthogonal  
Case Study: Detecting Redundant Sensor Readings Using Matrix Rank 
An industrial system uses four sensors ğ‘†1, ğ‘†2, ğ‘†3, ğ‘†4to monitor a physical process. 
For a certain time interval, the sensor readings (after normalization) are represented by the 
columns of the matrix 
ğ´ = [
1 2 3 1
2 4 6 2
1 2 3 1
] 
 
Each column corresponds to the readings of one sensor across three-time instants. Identify which 
sensor readings are redundant. Also, determine the minimum number of sensors required to 
represent all sensor information without loss. 
 
Basically, redundant means which is not necessary 
In data science, a redundant feature is a variable that provides no additional information to a 
model beyond what is already provided by other variables. 
Calling a blank sheet as empty 
Temperature of a room in K and Celsius. 
The Real-Life Example: The "Weather Station" 
Imagine you are building a machine learning model to predict Ice Cream Sales. You collect data 
from a weather station that gives you three features: 
1. Temperature in Celsius (x1) 
2. Temperature in Fahrenheit (x2) 
3. Wind Speed (x3) 
Why is there redundancy? 
The Temperature in Fahrenheit is a redundant feature. Why? Because it doesn't tell the model 
anything new that the Celsius column hasn't already said. They are just the same physical reality 
expressed in different scales. 
Redundant: Temperature in Fahrenheit (it's just a math transformation of Celsius). 
Useful/Unique: Wind Speed (it measures something entirely different from temperature). 
Methods to identify Redundancy: 
The most common way to identify and quantify redundancy is through mathematics, specifically 
Linear Dependence and Correlation Coefficients. 
Linear Dependence (Linear Algebra) 
In a matrix of data, features are represented as vectors. If one feature vector is a scalar multiple 
of another (e.g., V2 = 2 V1), they are linearly dependent. In mathematical terms, the "Rank" of 
your data matrix doesn't increase when you add a redundant feature. 
 
Solution: 
 
Concept: Matrix Rank, LI and LD. 
How? We will see 
First let us discuss the required concepts. 
 
Definition: Matrix Rank 
The rank of a matrix is defined as the maximum number of linearly independent rows or 
columns of the matrix. 
Equivalently, 
â€¢ Rank = dimension of the row space 
â€¢ Rank = dimension of the column space 
Key Points (Exam-Friendly) 
â€¢ Row rank = Column rank 
â€¢ Rank indicates the true dimensionality of the data 
â€¢ A matrix has full rank if its rank equals the smaller of the number of rows or columns 
Example 1: Matrix with Full Rank 
Consider the matrix 
ğ´ = [1 0
0 1] 
 
Rows are linearly independent and Columns are linearly independent, hence rank(ğ´) = 2 
 
Example 2: Matrix with Dependent Rows and Columns 
ğµ = [1 2
2 4] 
Second row = 2 Ã— first row  
Second column = 2 Ã— first column 
rank(ğµ) = 1 
 
Example 3: Rectangular Matrix 
ğ¶ = [1 2 3
2 4 6] 
 
â€¢ Second row is a multiple of the first 
â€¢ All columns depend on one vector 
rank(ğ¶) = 1 
Interpretation 
â€¢ High rank â†’ more independent information 
â€¢ Low rank â†’ redundancy present 
 
Consider the vectors in â„2: 
ğ‘£1 = [1
2] , ğ‘£2 = [3
6] 
Step 1: Form a Linear Combination 
Check whether there exist scalars ğ‘1, ğ‘2, not both zero, such that 
ğ‘1ğ‘£1 + ğ‘2ğ‘£2 = [0
0] 
 
That is, 
ğ‘1 [1
2] + ğ‘2 [3
6] = [0
0] 
Step 2: Solve the System 
ğ‘1 + 3ğ‘2 = 0
2ğ‘1 + 6ğ‘2 = 0 
 
The second equation is just twice the first, so there are infinitely many non-zero solutions, for 
example: 
ğ‘1 = âˆ’3, ğ‘2 = 1 
 
Case Study: Detecting Redundant Sensor Readings Using Matrix Rank 
An industrial system uses four sensors ğ‘†1, ğ‘†2, ğ‘†3, ğ‘†4to monitor a physical process.  
For a certain time interval, the sensor readings (after normalization) are represented by the columns 
of the matrix 
ğ´ = [
1 2 3 1
2 4 6 2
1 2 3 1
] 
 
Each column corresponds to the readings of one sensor across three -time instants. Identify which 
sensor readings are redundant. Also, determine the minimum number of sensors required to 
represent all sensor information without loss. 
Step 1: Observe Linear Dependence 
â€¢ Column 2 is 2 times column 1 
â€¢ Column 3 is 3 times column 1 
â€¢ Column 4 is equal to column 1 
Thus, all columns are scalar multiples of the first column. 
Step 2: Determine the Rank 
Since all columns depend on one column, 
rank(ğ´) = 1 
Step 3: Identify Redundant Sensors 
â€¢ Only one sensor provides independent information 
â€¢ Sensors ğ‘†2, ğ‘†3,and ğ‘†4are redundant 
Step 4: Minimum Sensors Required 
1 sensor  
 
is sufficient to represent all readings without information loss. 
Step 5: Interpretation 
Matrix rank reveals the true dimensionality of sensor data. 
A rank smaller than the number of sensors indicates redundancy. 
Final Answer 
Using matrix rank, redundant sensor readings are identified and removed, leading to efficient 
data representation and reduced computational cost. 
 
Case Study 2 : Removing Redundant Features in Smart City Air-Quality Monitoring 
A smart city installs multiple sensors at each monitoring station to assess air quality. 
For one station, the following six features are recorded every hour: 
1. ğ‘“1: Concentration of Carbon Monoxide (CO) 
2. ğ‘“2: Concentration of Nitrogen Oxides (NO) 
3. ğ‘“3: Air Quality Index (AQI) 
4. ğ‘“4: Weighted pollution score 
5. ğ‘“5: Temperature 
6. ğ‘“6: Heat index 
The city wants to reduce storage and computation costs by removing redundant features, 
without losing information. 
 
Mathematical Representation 
Each feature is represented as a column vector of sensor readings over time: 
ğ‘‹ = [ğ‘“1 â€Šğ‘“2 â€Šğ‘“3 â€Šğ‘“4 â€Šğ‘“5 â€Šğ‘“6] 
 
All vectors belong to the vector space â„ğ‘›, where ğ‘›is the number of time samples. 
Given Relationships (from environmental science) 
â€¢ AQI is computed using CO and NO 
â€¢ Weighted pollution score is a linear combination of AQI and pollutant concentrations 
â€¢ Heat index is computed from temperature 
Mathematically: 
ğ‘“3 = ğ‘ğ‘“1 + ğ‘ğ‘“2
ğ‘“4 = ğ‘ğ‘“1 + ğ‘‘ğ‘“2 + ğ‘’ğ‘“3
ğ‘“6 = ğ‘˜ğ‘“5
 
 
Step-by-Step Solution Using Mathematical Concepts 
Linear Independence (Detecting Redundancy) 
A set of vectors is linearly independent if none can be written as a linear combination of others. 
From the given relations: 
â€¢ ğ‘“3depends on ğ‘“1, ğ‘“2 
â€¢ ğ‘“4depends on ğ‘“1, ğ‘“2, ğ‘“3 
â€¢ ğ‘“6depends on ğ‘“5 
Hence, 
{ğ‘“1, ğ‘“2, ğ‘“3, ğ‘“4, ğ‘“5, ğ‘“6} 
 
is a linearly dependent set. 
How this helps? 
Linear dependence mathematically proves that some features do not add new information. 
Rank (Finding True Dimensionality) 
Although there are 6 features, only these are independent: 
{ğ‘“1, ğ‘“2, ğ‘“5} 
Therefore, 
rank(ğ‘‹) = 3 
 How this helps 
Rank tells us the true number of informative features, independent of how many are recorded. 
 
 Basis (Selecting Essential Features) 
A basis is a minimal linearly independent set that spans the same feature space. 
Choose the basis: 
â„¬ = {ğ‘“1 (CO), ğ‘“2 (NO), ğ‘“5 (Temperature)} 
 
All other features can be generated from this basis. 
How this helps 
Basis selection gives a systematic rule for which features to keep and which to remove. 
Orthogonality (Avoiding Overlap of Information) 
The original features are not orthogonal, because: 
â€¢ AQI overlaps with CO and NO 
â€¢ Heat index overlaps with temperature 
Non-orthogonal features have high correlation. 
By keeping only, the basis features (or orthogonalizing them if needed), each retained feature 
contributes distinct information. 
How this helps 
Orthogonality ensures that no retained feature duplicates information from another. 
Final Result: Redundant Feature Removal 
Removed Features 
â€¢ AQI 
â€¢ Weighted pollution score 
â€¢ Heat index 
Retained Features 
â€¢ CO concentration 
â€¢ NO concentration 
â€¢ Temperature 
Dimensionality Reduction:  6 â€Š â€Š â†’  â€Š â€Š3 
 
 
 
Final Conclusion : 
In this smart -city air-quality case study, redundant features arise because several sensor 
variables are linear combinations of others. Linear independence identifies redundancy, 
rank reveals true dimensionality, basis selection determines essential featur es, and 
orthogonality ensures non -overlapping information, enabling effective removal of 
redundant features without loss of data. 
 
Case study 3: A smart building uses four sensors to monitor environmental conditions. 
Each sensor records readings at three-time instants. 
The sensor data (after normalization) is represented by the matrix 
ğ´ = [
1 2 3 1
2 4 6 2
3 6 9 3
] 
 
Each column of the matrix corresponds to a sensor: 
â€¢ ğ‘†1, ğ‘†2, ğ‘†3, ğ‘†4 
The objective is to identify and remove redundant sensors without losing information. 
 


--- END OF Session-3-1.pdf ---



--- START OF Session-4 and 5.pdf ---

 
 
 
Case Study: Face Recognition using PCA (Eigenfaces Method) 
 
A university wants to build a face recognition system for secure access to a laboratory.  
The system uses Principal Component Analysis (PCA), also known as the Eigenfaces method, to 
recognize faces efficiently. 
Each face image is a grayscale image of size 64Ã—64 pixels. 
In this session we will discuss this case study in details. 
Session-4: Eigen values and Eigen vectors_ 
Introduction  
In linear algebra, eigenvalues arise when we study how a linear transformation acts on vectors. 
Normally, when a matrix multiplies a vector, both the magnitude and direction of the vector 
change. However, for certain special vectors, the direction remains the same â€”only the 
magnitude is scaled. 
 
These special vectors are called eigenvectors, and the corresponding scaling factors are called 
eigenvalues. 
Let ğ´ be a square matrix  of order ğ‘›Ã—ğ‘› . 
A scalar ğœ†is called an eigenvalue of matrix ğ´if there exists a non-zero vector ğ±â‰ ğŸsuch that 
ğ´ğ±=ğœ†ğ± 
 
Here: 
â€¢ ğ± is called an eigenvector corresponding to the eigenvalue ğœ†, 
â€¢ ğœ† represents the factor by which the eigenvector is scaled. 
Rewriting the equation: 
(ğ´âˆ’ğœ†ğ¼)ğ±=ğŸ 
 
For a non-trivial solution to exist, 
ğ‘‘ğ‘’ğ‘¡(ğ´âˆ’ğœ†ğ¼)=0 
 
 
Explanation: Characteristic equation  
For a 2x2 matrix 
Let A = 
11 12
21 22
aa
aa
ïƒ©ïƒ¹
ïƒªïƒºïƒ«ïƒ» be a square matrix of order 2x2. We can form the matrix 
AI ï¬âˆ’ , where 
ï¬ is 
a scalar. The determinant of a square matrix equated to zero. i.e., 
AI ï¬âˆ’ = 
11 12
21 22
aa
aa
ï¬
ï¬
âˆ’
âˆ’ =0 
is called the character equation of A. 

On expanding the determinant, the characteristic equation taken the form   
ğ€ğŸâˆ’ğ€(ğ’•ğ’“ğ’‚ğ’„ğ’† ğ’ğ’‡ ğ‘¨)+ğ’…ğ’†ğ’•ğ’†ğ’“ğ’ğ’Šğ’ğ’‚ğ’ğ’• ğ’ğ’‡ ğ‘¨=ğŸ. 
For a 3x3 matrix 
Let A = 
11 12 13
21 22 23
31 32 33
a a a
a a a
a a a
ïƒ©ïƒ¹
ïƒªïƒº
ïƒªïƒº
ïƒªïƒºïƒ«ïƒ»  be a square matrix of order 3x3. We can form the matrix 
AI ï¬âˆ’ , where 
ï¬
is a scalar. The determinant of a square matrix equated to zero. i.e., 
AI ï¬âˆ’ = 0 . On 
expanding the determinant, the characteristic equation taken the form  
ğ€ğŸ‘âˆ’ğ€ğŸ(ğ’•ğ’“ğ’‚ğ’„ğ’† ğ’ğ’‡ ğ‘¨)+ğ€(ğ’”ğ’–ğ’ ğ’ğ’‡ ğ’‘ğ’“ğ’Šğ’ğ’„ğ’Šğ’‘ğ’‚ğ’ ğ’ğ’Šğ’ğ’ğ’“ğ’” ğ’ğ’‡ ğ‘¨)âˆ’ğ’…ğ’†ğ’•ğ’†ğ’“ğ’ğ’Šğ’ğ’‚ğ’ğ’• ğ’ğ’‡ ğ‘¨=ğŸ. 
The roots of the characteristic equation are called the characteristic roots or latent roots or Eigen 
values of matrix A. 
Eigen vector: A non-zero vector ğ‘‹ is called an eigenvector of a matrix ğ´ associated with an 
eigen value ğœ† if ğ´ğ—=ğœ†ğ‘‹ holds. That is, 
An eigenvector of a square matrix ğ‘¨ is a non-zero vector ğ‘¿ such that when ğ‘¨ acts on it, the 
direction of ğ‘¿ does not changeâ€”it only gets scaled. 
Example: Consider the matrix 
ğ´=[2 1
1 2] 
The characteristic equation is det(Aâˆ’Î»I)=0 
ğœ†2âˆ’4ğœ†+3=0 
 Implies ğœ†=1,3. 
 
Step 3: Eigen Vector 
For ğœ†=3: 
(ğ´âˆ’3ğ¼)ğ±=0â‡’[âˆ’1 1
1 âˆ’1]ğ±=0 
This gives 
ğ±=[1
1] 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Example-2: Find the eigenvalues and eigenvectors of the matrix 
ğ´=[
2 1 1
1 2 1
1 1 2
]. 
 
Properties of Eigen values Eigen Values  
 
(1) The sum of the Eigen values of a matrix is the sum of the elements of the principal 
diagonal. 
(2) If ğœ† is an Eigen value of a matrix, A then 1/ğœ† is the Eigen value of ğ´âˆ’1. 
(3) If ğœ† is an Eigen value of an orthogonal matrix, then 1/ğœ† is also its Eigen value. 
(4) If ğœ†  be an Eigen value for a non â€“ singular matrix, A show that 
|ğ´|
ğœ†  is an Eigen value of 
the matrix (Adj A). 
(5) The Eigen values of triangular matrix A are equal to the elements of the principal 
diagonal of A. 
(6) If ğœ†1,ğœ†2,âˆ’âˆ’âˆ’âˆ’,ğœ†ğ‘›  are the Eigen values of a matrix A then ğ´ğ‘š has the Eigen values 
ğœ†1
ğ‘š ,ğœ†2
ğ‘š ,âˆ’âˆ’âˆ’ ğœ†ğ‘›
ğ‘š. 
(7) Any square matrix A and its transpose AT have same Eigen values. 
(8) The product of the Eigen values of a matrix is equal to its determinant. 
 
 
Definition of Principal Component Analysis (PCA): 
Principal Component Analysis (PCA)  is a statistical and linear algebra â€“based dimensionality 
reduction technique  that transforms a set of possibly correlated variables  into a new set of 
uncorrelated variables , called principal components , ordered so that the first few components 
retain most of the variation (information) present in the original data. 
Mathematically, the principal components are the eigenvectors of the covariance matrix  of the 
data, and the corresponding eigenvalues represent the amount of variance explained by each 
component. 
Case Study: Eigenvalues and Eigenvectors in Face Recognition (PCA / Eigenfaces) 
Introduction 
Eigenvalues and eigenvectors play a crucial role in dimensionality reduction, data compression, 
and pattern recognition. One of the most popular real -world applications is Face Recognition 
using Principal Component Analysis (PCA), commonly known as the Eigenfaces method. 
In this case study, we examine how eigenvalues and eigenvectors are used to extract meaningful 
features from facial images while reducing computational complexity. 
 
 
Conceptual Diagram (Eigenfaces Method) 
 
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 
â”‚  Face 1           â”‚  â”‚  Face 2            â”‚   â”‚       Face 3      â”‚ 
â”‚ (64Ã—64)         â”‚   â”‚ (64Ã—64)          â”‚   â”‚       (64Ã—64)    â”‚ 
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 
        â”‚                               â”‚                             â”‚ 
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 
                                       â†“ 
        Vectorization (4096 Ã— 1 each) 
                                       â†“ 
        Mean Face Subtraction 
                                       â†“ 
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 
     â”‚   Covariance Matrix                      â”‚ 
     â”‚   (via Aáµ€A trick)                              â”‚ 
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 
                                      â†“ 
        Eigenvectors (Eigenfaces) 
                                     â†“ 
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 
     â”‚Eigenface1    â”‚Eigenface2    â”‚Eigenface3    â”‚ 
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 
                                     â†“ 
     Projection into k-D Eigenspace 
                                     â†“ 
        â—      â—        â—      (Training faces) 
 
------------------------------------------------ 
 
          Test Face Image (64Ã—64) 
                    â”‚ 
              Mean Subtraction 
                    â”‚ 
              Projection (â—) 
                    â”‚ 
      Distance Comparison in Eigenspace 
                    â”‚ 
              Closest Match Found 
                    â”‚ 
           Face Recognized 
 
 
Challenges: 
â€¢ High memory usage 
â€¢ Slow computation 
â€¢ Redundant information 
Goal: 
Reduce the dimensionality of face images while preserving the most important facial features. 
Why Eigenvalues and Eigenvectors? 
â€¢ Eigenvectors represent directions of maximum variance in data 
â€¢ Eigenvalues indicate how important each direction is 
â€¢ PCA uses eigenvectors of the covariance matrix 
 In face recognition, these eigenvectors are called Eigenfaces. 
Methodology 
Step 1: Data Collection 
â€¢ Collect facial images of different people 
â€¢ Convert images to grayscale 
â€¢ Flatten each image into a vector 
Example: 
100Ã—100â†’10,000 dimensional vector 
Step 2: Mean Centering 
Compute the mean face and subtract it from each image vector: 
Xcentered=Xâˆ’Î¼ 

This ensures data is centered around the origin. 
Step 3: Covariance Matrix 
C=1
nXcentered
T Xcentered 
The covariance matrix captures relationships between pixels. 
Step 4: Eigen value Decomposition 
Solve: 
CX=Î»X 
 
Where: 
â€¢ Xâ†’ eigenvector (Eigenface) 
â€¢ Î»â†’ eigenvalue (importance) 
Step 5: Feature Selection 
â€¢ Sort eigenvalues in descending order 
â€¢ Select top k eigenvectors 
â€¢ These eigenvectors form a new feature space 
 Large eigenvalues = important facial patterns 
 Small eigenvalues = noise 
Step 6: Projection 
Project face images onto eigenfaces: 
Y=XTU 
This gives a compact representation of faces. 
Results 
Aspect Before PCA After PCA 
Dimensions 10,000 100â€“300 
Storage High Low 
Speed Slow Fast 
Accuracy Moderate High 
â€¢ Most facial information is captured using very few eigenvectors 
â€¢ System becomes faster and more efficient 
6. Interpretation of Eigenvalues and Eigenvectors 
â€¢ Eigenvectors = Facial patterns (eyes, nose, mouth structures) 
â€¢ Eigenvalues = Amount of variance captured by each pattern 
â€¢ Larger eigenvalue â†’ more important eigenface 
 
Discussion about above mentioned cases study. For convenient purpose we are taking the 
case of 3 images. 
Training Images (Grayscale) 
Assume 3 training face images: 
Image 1 
ğ¼1=[
1 2 3
2 3 4
3 4 5
] 
 
Image 2 
ğ¼2=[
2 3 4
3 4 5
4 5 6
] 
 
Image 3 
ğ¼3=[
3 4 5
4 5 6
5 6 7
] 
Step 1: Vectorization 
Each 3Ã—3image is converted into a 9Ã—1column vector: 
x1=
(
 
 
 
 
 
 
1
2
3
2
3
4
3
4
5)
 
 
 
 
 
 
,x2=
(
 
 
 
 
 
 
2
3
4
3
4
5
4
5
6)
 
 
 
 
 
 
,x3=
(
 
 
 
 
 
 
3
4
5
4
5
6
5
6
7)
 
 
 
 
 
 
 
Step 2: Mean Face 
ğœ‡=1
3(x1+x2+x3)=
(
 
 
 
 
 
 
2
3
4
3
4
5
4
5
6)
 
 
 
 
 
 
 
 
Step 3: Mean-Centered Images 
ğœ™ğ‘–=xğ‘–âˆ’ğœ‡ 
 ğœ™1=
(
 
 
 
 
 
 
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1)
 
 
 
 
 
 
,ğœ™2=
(
 
 
 
 
 
 
0
0
0
0
0
0
0
0
0)
 
 
 
 
 
 
 ğ‘ğ‘›ğ‘‘ ğœ™3=
(
 
 
 
 
 
 
1
1
1
1
1
1
1
1
1)
 
 
 
 
 
 
. 
Step 4: Data Matrix 
ğ´=[
âˆ’1 0 1
âˆ’1 0 1
âˆ’1 0 1
]
9Ã—3
 
Step 5: Compute ğ‘³=ğ‘¨ğ‘»ğ‘¨ 
ğ¿=(
9 0 âˆ’9
0 0 0
âˆ’9 0 9
)  
Step 6: Eigenvalues and Eigenvectors 
Eigenvalues: 
ğœ†1=18,ğœ†2=0,ğœ†3=0 
 
Corresponding eigenvector for ğœ†1: ğ¯1=(
âˆ’1
0
1
)  
Step 7: Eigenface Construction ğ®1=ğ´ğ¯1=
(
 
 
 
 
 
 
2
2
2
2
2
2
2
2
2)
 
 
 
 
 
 
 
 
Normalize: 
ğ®1=1
6
(
 
 
 
 
 
 
2
2
2
2
2
2
2
2
2)
 
 
 
 
 
 
=
(
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3)
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
. 
Step 8: Projection of Training Faces 
ğœ”ğ‘–=ğ®1
ğ‘‡ğ“ğ‘– 
 
Face Projection 
Face 1 -3 
Face 2 0 
Face 3 3 
  
Step 9: Test Face Recognition Test Image  ğ¼ğ‘¡ğ‘’ğ‘ ğ‘¡=[
2 3 4
3 4 5
4 5 6
] 
 
Vectorized: xğ‘¡ğ‘’ğ‘ ğ‘¡=
(
 
 
 
 
 
 
2
3
4
3
4
5
4
5
6)
 
 
 
 
 
 
 
Mean-centered: 
ğ“ğ’•ğ’†ğ’”ğ’•=ğŸ 
 
Projection: 
ğğ’•ğ’†ğ’”ğ’•=ğŸ 
Recognition Decision 
Distances: 
âˆ£0âˆ’0âˆ£=0(minimum) 
Test face is recognized as Face 2. 
 
 Applications Beyond Face Recognition 
â€¢ Image compression 
â€¢ Noise reduction 
â€¢ Recommendation systems 
â€¢ Feature extraction in Machine Learning 
â€¢ Deep Learning preprocessing (CNN inputs) 
 
Problem: Face Recognition using PCA (Eigenfaces) 
A database contains 5 grayscale face images, each of size 2Ã—2pixels. 
Each image is converted into a 4-dimensional column vector by stacking its pixels column-wise. 
The face vectors are: x1=(
1
2
3
4
) ,x2=(
2
3
4
5
) ,x3=(
3
4
5
6
) ,x4=(
4
5
6
7
) ,x5=(
5
6
7
8
)  
 
A test face image is given by: x3=(
3
4
5
6
) . 
 
 
Singular Value Decomposition (SVD) 
Singular Value Decomposition (SVD) is a mathematical technique widely used in machine 
learning for tasks like dimensionality reduction, noise reduction, and data compression. By 
breaking down a matrix into its fundamental components, SVD helps uncover patterns in data, 
making it easier to analyse and process large datasets. 
Purpose of SVD in Machine Learning: 
SVD enables the simplification of complex datasets by: 
â€¢ Reducing dimensionality while retaining key information. 
â€¢ Enhancing model performance by removing noise. 
â€¢ Facilitating data compression for efficient storage. 
Mathematics Behind SVD Algorithm 
Singular Value Decomposition (SVD) is a mathematical process that decomposes a matrix into 
three distinct matrices: U, Î£, and Váµ€. This decomposition is the foundation of its applications 
in machine learning, allowing for efficient data transformation and analysis. 
Definition and Components: 
For a matrix A with dimensions mÃ—n, SVD is represented as: ğ´ = ğ‘ˆï“ ğ‘‰ğ‘‡ 
where: 
U (Left Singular Vectors): An mÃ—m orthogonal matrix representing the row space of A. 
Î£ ( Singular Values): A diagonal  mÃ—n matrix containing non -negative singular values, 
which represent the importance or weight of corresponding dimensions. 
Váµ€ (Right Singular Vectors): An nÃ—n orthogonal matrix representing the column space of 
A. 
Geometric Interpretation: 
SVD geometrically transforms a dataset by: 
1. Rotation: Aligning the data along its principal directions (defined by U and V). 
2. Scaling: Adjusting the data based on the singular values in ï“. 
This transformation helps identify the most significant features or patterns in the data, making 
it easier to process. 
Relation to Eigenvalues and Eigenvectors: 
SVD is closely related to eigen decomposition, a technique used to diagonalize square matrices: 
â€¢ Forğ´ğ‘‡ğ´ , the eigenvectors form V, and the square roots of eigenvalues are the singular 
values. 
â€¢ For ğ´ğ´ğ‘‡ , the eigenvectors form U, and the singular values remain the same. 
Key Difference: 
â€¢ Eigen decomposition works only for square matrices, while SVD applies to rectangular 
matrices, making it more versatile for real-world applications. 
Singular Value Decomposition Example 
The process of finding the singular value decomposition for 3Ã—3 matrix and 2Ã—2 matrix is the 
same. Letâ€™s have a look at the example of 2Ã—2 matrix decomposition. 
Singular Value Decomposition  
2Ã—2 Matrix Example 
Example 1: Obtain the singular value decomposition of a matrix ğ‘¨ = [âˆ’ğŸ’ âˆ’ğŸ•
ğŸ ğŸ’ ] 

Solution: Give that the matrix ğ´ = [âˆ’4 âˆ’7
1 4 ]. 
Step 1: Compute ğ‘¨ğ‘»ğ‘¨  
ğ´ğ‘‡ğ´ = [âˆ’4 1
âˆ’7 4][âˆ’4 âˆ’7
1 4 ] = [17 32
32 65]. 
Step 2: Eigenvalues of ğ‘¨ğ‘»ğ‘¨ 
Solve det (ğ´ğ‘‡ğ´ âˆ’ ğœ†ğ¼) = 0: 
ğœ†2 âˆ’ 82ğœ† + 81 = 0 
â‡’ ğœ†1 = 81,ğœ†2 = 1. 
Step 3: Singular values 
ğœ1 = âˆš81 = 9,ğœ2 = âˆš1 = 1. 
So, 
Î£ = [9 0
0 1]. 
Step 4: Right singular vectors (ğ‘½) 
Eigenvectors of ğ´ğ‘‡ğ´: 
â€¢ For ğœ†1 = 81: eigenvector (1,2) 
â€¢ For ğœ†2 = 1: eigenvector (âˆ’2,1) 
Normalize: 
ğ‘£1 = 1
âˆš5
[1
2],ğ‘£2 = 1
âˆš5
[âˆ’2
1 ]. 
ğ‘‰ = 1
âˆš5
[1 âˆ’2
2 1 ]. 
Step 5: Left singular vectors (ğ‘ˆ) 
ğ‘¢ğ‘– = 1
ğœğ‘–
ğ´ğ‘£ğ‘– 
ğ‘¢1 = 1
âˆš5
[âˆ’2
1 ], ğ‘¢2 = 1
âˆš5
[âˆ’1
âˆ’2]. 
ğ‘ˆ = 1
âˆš5
[âˆ’2 âˆ’1
1 âˆ’2]. 
 
 Final SVD 
ğ´ = ğ‘ˆÎ£ğ‘‰ğ‘‡  
 
where 
ğ‘ˆ = 1
âˆš5
[âˆ’2 âˆ’1
1 âˆ’2],Î£ = [9 0
0 1],ğ‘‰ğ‘‡ = 1
âˆš5
[ 1 2
âˆ’2 1]. 
Numerical verification of SVD 
From the previous result: 
ğ‘ˆ = 1
âˆš5
[âˆ’2 âˆ’1
1 âˆ’2],Î£ = [9 0
0 1],ğ‘‰ğ‘‡ = 1
âˆš5
[ 1 2
âˆ’2 1] 
 
Now compute ğ‘ˆÎ£: 
ğ‘ˆÎ£ = 1
âˆš5
[âˆ’18 âˆ’1
9 âˆ’2] 
 
Then 
ğ´ = ğ‘ˆÎ£ğ‘‰ğ‘‡ = 1
5[âˆ’18 âˆ’1
9 âˆ’2][ 1 2
âˆ’2 1] 
 
Multiplying: 
= 1
5[âˆ’16 âˆ’35
5 20 ] = [âˆ’4 âˆ’7
1 4 ] 
 
Verified numerically â€” the SVD is correct. 
Rank-1 approximation (dominant singular value) 
The rank-1 approximation is: 
ğ´1 = ğœ1â€‰ğ‘¢1ğ‘£1
ğ‘‡ 
where 
ğœ1 = 9,ğ‘¢1 = 1
âˆš5
[âˆ’2
1 ],ğ‘£1 = 1
âˆš5
[1
2] 
Step 1: Compute ğ’–ğŸğ’—ğŸ
ğ‘» 
ğ‘¢1ğ‘£1
ğ‘‡ = 1
5[âˆ’2 âˆ’4
1 2 ] 
Step 2: Multiply by ğˆğŸ = ğŸ— 
ğ´1 = 9
5[âˆ’2 âˆ’4
1 2 ] = [âˆ’3.6 âˆ’7.2
1.8 3.6 ] 
 Final Results 
Exact matrix 
ğ´ = [âˆ’4 âˆ’7
1 4 ] 
Rank-1 approximation 
ğ´1 = [âˆ’3.6 âˆ’7.2
1.8 3.6 ]  
 
Exmple-2: Obtain SVD of the matrix 
ğ‘¨ = [
ğŸ‘ ğŸ ğŸ
ğŸ ğŸ ğŸ
ğŸ ğŸ ğŸ
] 
 
This is a simple 3D example (diagonal matrix), but it perfectly illustrates how SVD works. 
Step 1: Definition of SVD 
For any matrix ğ´, 
ğ´ = ğ‘ˆÎ£ğ‘‰ğ‘‡ 
 
where 
â€¢ ğ‘ˆ= left singular vectors (orthonormal) 
â€¢ Î£= singular values (diagonal, non-negative) 
â€¢ ğ‘‰= right singular vectors (orthonormal) 
 Step 2: Compute ğ‘¨ğ‘»ğ‘¨ 
ğ´ğ‘‡ğ´ = [
9 0 0
0 4 0
0 0 1
] 
Step 3: Singular Values 
Eigenvalues of ğ´ğ‘‡ğ´are: 
ğœ†1 = 9,ğœ†2 = 4,ğœ†3 = 1 
 
Singular values: 
ğœ1 = 3,ğœ2 = 2,ğœ3 = 1 
 
So, 
Î£ = [
3 0 0
0 2 0
0 0 1
] 
 Step 4: Right Singular Vectors (ğ‘½) 
Since ğ´ğ‘‡ğ´is diagonal, eigenvectors are the standard basis vectors: 
ğ‘‰ = ğ¼ = [
1 0 0
0 1 0
0 0 1
] 
Step 5: Left Singular Vectors (ğ‘¼) 
ğ‘ˆ = ğ´ğ‘‰Î£âˆ’1 = ğ¼ 
Final SVD 
ğ´ = [
1 0 0
0 1 0
0 0 1
]
âŸ
ğ‘ˆ
[
3 0 0
0 2 0
0 0 1
]
âŸ
Î£
[
1 0 0
0 1 0
0 0 1
]
âŸ
ğ‘‰ğ‘‡
 
 
Geometric Interpretation (3D) 
â€¢ ğ‘‰ğ‘‡: rotates the 3D coordinate system 
â€¢ Î£: stretches space by 3, 2, and 1 along three orthogonal directions 
â€¢ ğ‘ˆ: rotates the result again 
This shows how SVD decomposes a 3D transformation into rotation â†’ scaling â†’ rotation. 
Rank-1 Approximation (Optional) 
Keeping only the largest singular value: 
ğ´1 = 3[
1
0
0
][1 0 0] = [
3 0 0
0 0 0
0 0 0
] 
 
Used in 3D data compression and noise reduction. 
 
Example 3: Given 3D Matrix 
ğ´ = [
1 1 0
1 1 0
0 0 0
] 
 
This kind of matrix appears in 3D data / image slices / correlated features. 
Solution: 
 Step 1: Compute ğ‘¨ğ‘»ğ‘¨ 
ğ´ğ‘‡ğ´ = [
2 2 0
2 2 0
0 0 0
] 
Step 2: Eigenvalues of ğ‘¨ğ‘»ğ‘¨ 
Solve det (ğ´ğ‘‡ğ´ âˆ’ ğœ†ğ¼) = 0 
Eigenvalues: 
ğœ†1 = 4,ğœ†2 = 0,ğœ†3 = 0 
Step 3: Singular Values 
ğœ1 = âˆš4 = 2,ğœ2 = 0,ğœ3 = 0 
So, 
Î£ = [
2 0 0
0 0 0
0 0 0
] 
Rank of A = 1 
Step 4: Right Singular Vectors ğ‘½ 
Eigenvector for ğœ†1 = 4: 
ğ‘£1 = 1
âˆš2
[
1
1
0
] 
Complete orthonormal basis: 
ğ‘‰ =
[
 
 
 
 1
âˆš2
1
âˆš2
0
1
âˆš2
âˆ’ 1
âˆš2
0
0 0 1]
 
 
 
 
 
Step 5: Left Singular Vectors ğ‘¼ 
ğ‘¢1 = ğ´ğ‘£1
ğœ1
= 1
2[
2
2
0
] = 1
âˆš2
[
1
1
0
] 
So, 
ğ‘ˆ =
[
 
 
 
 1
âˆš2
1
âˆš2
0
1
âˆš2
âˆ’ 1
âˆš2
0
0 0 1]
 
 
 
 
 
 Final SVD Decomposition 
ğ´ = ğ‘ˆÎ£ğ‘‰ğ‘‡ 
ğ´ =
[
 
 
 
 1
âˆš2
1
âˆš2
0
1
âˆš2
âˆ’ 1
âˆš2
0
0 0 1]
 
 
 
 
[
2 0 0
0 0 0
0 0 0
]
[
 
 
 
 1
âˆš2
1
âˆš2
0
1
âˆš2
âˆ’ 1
âˆš2
0
0 0 1]
 
 
 
 
ğ‘‡
 
 
Rank-1 Approximation (3D Compression) 
Since only one singular value is non-zero: 
ğ´ â‰ˆ ğœ1ğ‘¢1ğ‘£1
ğ‘‡ 
ğ´ = 2( 1
âˆš2
[
1
1
0
]) ( 1
âˆš2
[1 1 0]) 
Exact reconstruction using one direction only 
Geometric Meaning in 3D 
â€¢ All data lies along one direction 
â€¢ Other two dimensions contain no new information 
â€¢ Perfect example of dimensionality reduction from 3D â†’ 1D 
Example 4: Given 3D Rectangular Matrix ( ğŸ‘ Ã— ğŸ) 
ğ´ = [
1 0
0 1
1 1
] 
This represents 3D data with 2 features (very common in ML and image processing). 
Step 1: Dimensions 
â€¢ ğ´is 3 Ã— 2 
â€¢ ğ‘ˆâ†’ 3 Ã— 3 
â€¢ Î£â†’ 3 Ã— 2 
â€¢ ğ‘‰ğ‘‡â†’ 2 Ã— 2 
 Step 2: Compute ğ‘¨ğ‘»ğ‘¨ 
ğ´ğ‘‡ğ´ = [2 1
1 2] 
Step 3: Eigenvalues of ğ‘¨ğ‘»ğ‘¨ 
det (ğ´ğ‘‡ğ´ âˆ’ ğœ†ğ¼) = 0 
(2 âˆ’ ğœ†)2 âˆ’ 1 = 0 
ğœ†1 = 3,ğœ†2 = 1 
Step 4: Singular Values 
ğœ1 = âˆš3,ğœ2 = 1 
Î£ = [
âˆš3 0
0 1
0 0
] 
Step 5: Right Singular Vectors ğ‘½ 
Eigenvectors of ğ´ğ‘‡ğ´: 
For ğœ†1 = 3: 
ğ‘£1 = 1
âˆš2
[1
1] 
 
For ğœ†2 = 1: 
ğ‘£2 = 1
âˆš2
[ 1
âˆ’1] 
ğ‘‰ =
[
 
 
 1
âˆš2
1
âˆš2
1
âˆš2
âˆ’ 1
âˆš2]
 
 
 
 
Step 6: Left Singular Vectors ğ‘¼ 
ğ‘¢ğ‘– = ğ´ğ‘£ğ‘–
ğœğ‘–
 
ğ‘¢1 = 1
âˆš6
[
1
1
2
],ğ‘¢2 = 1
âˆš2
[
1
âˆ’1
0
] 
 
Complete with a third orthogonal vector: 
ğ‘¢3 = 1
âˆš3
[
âˆ’1
âˆ’1
1
] 
Final SVD 
ğ´ = ğ‘ˆÎ£ğ‘‰ğ‘‡ 
 
Where: 
ğ‘ˆ =
[
 
 
 
 
 
 1
âˆš6
1
âˆš2
âˆ’ 1
âˆš3
1
âˆš6
âˆ’ 1
âˆš2
âˆ’ 1
âˆš3
2
âˆš6
0 1
âˆš3 ]
 
 
 
 
 
 
 
Rank of the Matrix 
Rank(ğ´) = 2 
(Number of non-zero singular values) 
Rank-1 Approximation (Dimensionality Reduction) 
ğ´1 = ğœ1ğ‘¢1ğ‘£1
ğ‘‡ 
ğ´1 = [
0.5 0.5
0.5 0.5
1 1
] 
Reduced from 3D â†’ 1D dominant direction 
Geometric Interpretation (3D) 
â€¢ Data lives mainly on a plane 
â€¢ SVD finds two orthogonal directions 
â€¢ Third direction contains no information 
Example-5: Obtain SVD of ğ´ = [
2 1
1 2
2 2
] 
 
Solution:  Step 1: Compute ğ‘¨ğ‘»ğ‘¨ 
ğ´ğ‘‡ğ´ = [2 1 2
1 2 2][
2 1
1 2
2 2
] = [9 8
8 9] 
Step 2: Eigenvalues of ğ‘¨ğ‘»ğ‘¨ 
det (ğ´ğ‘‡ğ´ âˆ’ ğœ†ğ¼) = 0 
âˆ£ 9 âˆ’ ğœ† 8
8 9 âˆ’ ğœ† âˆ£= (9 âˆ’ ğœ†)2 âˆ’ 64 = 0 
ğœ†2 âˆ’ 18ğœ† + 17 = 0 
ğœ†1 = 17,ğœ†2 = 1 
Step 3: Singular Values 
ğœ1 = âˆš17,ğœ2 = 1 
Î£ = [
âˆš17 0
0 1
0 0
] 
Step 4: Right Singular Vectors ğ‘½ 
For ğ€ğŸ = ğŸğŸ• 
(ğ´ğ‘‡ğ´ âˆ’ 17ğ¼)ğ‘£ = 0 â‡’ [âˆ’8 8
8 âˆ’8]ğ‘£ = 0 
ğ‘£1 = 1
âˆš2
[1
1] 
 
For ğ€ğŸ = ğŸ 
ğ‘£2 = 1
âˆš2
[ 1
âˆ’1] 
ğ‘‰ =
[
 
 
 1
âˆš2
1
âˆš2
1
âˆš2
âˆ’ 1
âˆš2]
 
 
 
 
Step 5: Left Singular Vectors ğ‘¼ 
ğ‘¢ğ‘– = ğ´ğ‘£ğ‘–
ğœğ‘–
 
First left singular vector ğ’–ğŸ 
ğ´ğ‘£1 =
1
âˆš2[
3
3
4
],ğ‘¢1 =
1
âˆš34[
3
3
4
]  
Second left singular vector ğ’–ğŸ 
ğ´ğ‘£2 =
1
âˆš2[
1
âˆ’1
0
], ğ‘¢2 =
1
âˆš2[
1
âˆ’1
0
] , ğ´ğ‘£3 = 
Third vector ğ’–ğŸ‘(orthogonal completion) 
ğ‘¢3 = 1
âˆš17
[
âˆ’2
âˆ’2
3
] 
 
 Final SVD 
ğ´ = ğ‘ˆÎ£ğ‘‰ğ‘‡ 
 
where 
ğ‘ˆ =
[
 
 
 
 
 
 3
âˆš34
1
âˆš2
âˆ’ 2
âˆš17
3
âˆš34
âˆ’ 1
âˆš2
âˆ’ 2
âˆš17
4
âˆš34
0 3
âˆš17 ]
 
 
 
 
 
 
 
Rank of the Matrix 
Rank(ğ´) = 2 (two non-zero singular values) 
Rank-1 Approximation (Compression) 
ğ´1 = ğœ1ğ‘¢1ğ‘£1
ğ‘‡ = âˆš17
2 [
3 3
3 3
4 4
] 
Practice Problems on Singular Value Decomposition  
1. Obtain the SVD of the matrix A = [[2, 0], [0, 1]]. Also determine its rank.  
2. Given A = [[1, 0], [0, 0]]: 
(a) Find the singular values 
(b) Construct the rank-1 approximation 
(c) Interpret the result. 
3. Consider A = [[1, 1], [1, 1]]: 
(a) Compute the SVD 
(b) Find the rank 
(c) Explain why one singular value is zero. 
4. Compute the SVD of the rectangular matrix A = [[1, 0], [0, 1], [0, 0]]: 
(a) Find U, Î£, V^T 
(b) Determine the rank. 
5. Let A = [[1, 1], [0, 1], [1, 0]]: 
(a) Compute A^T A 
(b) Find singular values 
(c) Explain the geometric meaning. 
6. Given A = [[2, 1], [1, 2], [2, 2]]: 
(a) Find the singular values 
(b) Determine the rank 
(c) Construct the rank-1 approximation. 
7. A grayscale image is represented by A = [[3, 2, 1], [2, 1, 0], [1, 0, 0]]: 
(a) Compute the SVD 
(b) Determine the rank 
(c) Explain image compression using SVD. 
 
 
Numerical Example: PCA using Eigenvalues & Eigenvectors 
Given Data Matrix 
Assume we have 3 face images , each represented using 2 features  
(e.g., simplified pixel intensities). 
ğ‘‹ = [
2 4
4 6
6 8
] 
Rows â†’ Images, Columns â†’ Features 
Step 1: Compute the Mean Vector 
Mean of each column: 
ğœ‡1 = 2 + 4 + 6
3 = 4ğœ‡2 = 4 + 6 + 8
3 = 6 
ğœ‡ = [4 6] 
 
Step 2: Mean Center the Data 
ğ‘‹ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘ = ğ‘‹ âˆ’ ğœ‡ 
= [
2 âˆ’ 4 4 âˆ’ 6
4 âˆ’ 4 6 âˆ’ 6
6 âˆ’ 4 8 âˆ’ 6
] = [
âˆ’2 âˆ’2
0 0
2 2
] 
Step 3: Compute the Covariance Matrix 
ğ¶ = 1
ğ‘›âˆ’ 1ğ‘‹ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘
ğ‘‡ ğ‘‹ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘ 
ğ¶ = 1
2[âˆ’2 0 2
âˆ’2 0 2][
âˆ’2 âˆ’2
0 0
2 2
] 
ğ¶ = 1
2[8 8
8 8] = [4 4
4 4] 
Step 4: Find Eigenvalues 
Solve: 
âˆ£ ğ¶ âˆ’ ğœ†ğ¼ âˆ£= 0 
âˆ£ 4 âˆ’ ğœ† 4
4 4 âˆ’ ğœ† âˆ£= 0 
(4 âˆ’ ğœ†)2 âˆ’ 16 = 0 
ğœ†2 âˆ’ 8ğœ† = 0 â‡’ ğœ†(ğœ† âˆ’ 8) = 0 
ğœ†1 = 8, â€Šğœ†2 = 0  
Step 5: Find Eigenvectors 
For ğ€ğŸ = ğŸ– 
(ğ¶ âˆ’ 8ğ¼)ğ‘£ = 0 â‡’ [âˆ’4 4
4 âˆ’4][ğ‘¥
ğ‘¦] = 0 
ğ‘¥ = ğ‘¦ â‡’ ğ‘£1 = [1
1] 
 
Normalize: 
ğ‘£1 = 1
âˆš2
[1
1] 
For ğ€ğŸ = ğŸ 
ğ‘¥ = âˆ’ğ‘¦ â‡’ ğ‘£2 = [ 1
âˆ’1] 
Step 6: Principal Component (PCA Direction) 
â€¢ Largest eigenvalue: 8 
â€¢ Corresponding eigenvector: 
Principal Component = 1
âˆš2
[1
1]  
This is the Eigenface direction in simplified form. 
Step 7: Project Data onto Principal Component 
ğ‘ = ğ‘‹ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘ â‹… ğ‘£1 
ğ‘ = [
âˆ’2 âˆ’2
0 0
2 2
] â‹… 1
âˆš2
[1
1] = [
âˆ’2âˆš2
0
2âˆš2
] 
Data reduced from 2D â†’ 1D 
Step 8: Interpretation (Face Recognition Context) 
â€¢ Eigenvalue 8 â†’ major facial variation 
â€¢ Eigenvalue 0 â†’ redundant feature 
â€¢ PCA removes redundancy and noise 
â€¢ Projection coefficients are used for face matching 
Final Exam-Ready Conclusion 
Using PCA, the original 2 -dimensional data is reduced to 1 dimension by selecting the 
eigenvector corresponding to the largest eigenvalue. This principal component captures 
maximum variance and forms the basis of the Eigenfaces method in face recognition. 
 
Rank-1 Reconstruction (Using PCA) 
From the previous result: 
Principal eigenvector: ğ‘£1 =
1
âˆš2[1
1] 
Projected values: ğ‘ = [
âˆ’2âˆš2
0
2âˆš2
] 
Rank-1 Approximation Formula 
ğ‘‹ğ‘Ÿğ‘ğ‘›ğ‘˜1 = ğ‘ğ‘£1
ğ‘‡ 
Add Mean Back 
Mean vector: 
ğœ‡ = [4 6] 
= [
âˆ’2âˆš2
0
2âˆš2
] â‹… 1
âˆš2
[1 1] = [
âˆ’2 âˆ’2
0 0
2 2
] ğ‘‹ğ‘Ÿğ‘’ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘’ğ‘‘ = [
2 4
4 6
6 8
] 
Perfect reconstruction here because data is already rank-1. 
Interpretation 
â€¢ Only one eigenvalue is non-zero 
â€¢ Data lies in one principal direction 
â€¢ PCA achieves maximum compression without loss 
Eigenfaces using ğ‘¨ğ‘»ğ‘¨ Trick (Numerical) Image size â‰« number of images 
Step 1: Mean-centered matrix 
ğ´ = [
âˆ’2 âˆ’2
0 0
2 2
] 
Step 3: Eigenvalues ğ´ğ‘‡ğ´ ğ‘ğ‘Ÿğ‘’  
ğœ†1 = 16,ğœ†2 = 0 
Step 2: Compute ğ‘¨ğ‘»ğ‘¨ 
ğ´ğ‘‡ğ´ = [âˆ’2 0 2
âˆ’2 0 2][
âˆ’2 âˆ’2
0 0
2 2
] = [8 8
8 8] 
|ğ´ğ‘‡ğ´ âˆ’ ï¬ğ¼| = |[8 8
8 8] âˆ’ ï¬[1 0
0 1]| = 0 
Step 4: Eigenvector 
ğ‘£1 = 1
âˆš2
[1
1] 
Step 5: Compute Eigenface 
ğ‘¢1 = ğ´ğ‘£1 = [
âˆ’2 âˆ’2
0 0
2 2
] 1
âˆš2
[1
1] = [
âˆ’2âˆš2
0
2âˆš2
] 
This vector reshaped â†’ Eigenface image  
Distance-Based Face Recognition (Numerical) 
Training weights:   ğœ”1 = âˆ’2âˆš2,ğœ”2 = 0,ğœ”3 = 2âˆš2 
Test face: Î“ğ‘¡ğ‘’ğ‘ ğ‘¡ = [5 7] and Mean-center: Î¦ğ‘¡ğ‘’ğ‘ ğ‘¡ = [1 1] 
Projection: ğœ”ğ‘¡ğ‘’ğ‘ ğ‘¡ = [1 1]â‹…
1
âˆš2[1
1] = âˆš2 
Euclidean Distance 
ğ‘‘1 =âˆ£ âˆš2 + 2âˆš2 âˆ£= 3âˆš2 
ğ‘‘2 =âˆ£ âˆš2 âˆ’ 0 âˆ£= âˆš2 
ğ‘‘3 =âˆ£ âˆš2 âˆ’ 2âˆš2 âˆ£= âˆš2 
 
      Closest match â†’ Face 2 or Face 3 
 
 
Face Images 
     â†“ 
Vectorization 
     â†“ 
Mean Face Computation 
     â†“ 
Mean Subtraction 
     â†“ 
Covariance Matrix (Aáµ€A) 
     â†“ 
Eigenvalues & Eigenvectors 
     â†“ 
Eigenfaces Selection 
     â†“ 
Projection (Weights) 
     â†“ 
Distance Comparison 
     â†“ 
Face Recognition 


--- END OF Session-4 and 5.pdf ---



--- START OF HA-1.pdf ---

CO-1: Home Assignment-1 
 
1. Identify the combined embedding feature vector from vectors ğ‘’âƒ—1 = (0,1,2), ğ‘’âƒ—2 = (1, âˆ’1,2), 
ğ‘’âƒ—3 = (âˆ’1,0,2).  
2. In a neural network layer, if the input vector is ğ± âˆˆ â„3and weight matrix is ğ‘Š âˆˆ â„2Ã—3, 
Identify the dimension of the output vector. 
3. Determine the norms of u and v, and evaluate u â‹… v, if u = (ğ‘, 2ğ‘, 3ğ‘, â€¦ , ğ‘›ğ‘) and v =
(ğ‘, ğ‘, ğ‘, â€¦ , ğ‘). 
4. A geometric transformation is represented by the matrix ğ´ = [1 0
0 2]. If a planar region has 
an original area of 6 square units, compute the area of the region after the transformation. 
5. An online streaming platform analyzes movie descriptions to improve its recommendation 
system by identifying similarities in plot content. Two movie plots are given: 
Movie A: A young wizard discovers his powers at a magic school. 
Movie B: A boy with magical abilities studies spells in a wizard school.  
Analyze the textual similarity between the two movie plots by applying the TF -IDF 
representation followed by cosine similarity computation also examine the resulting cosine 
similarity value and interpret what it reveals about the degree of thematic and con textual 
similarity between the two movie plots in the context of recommendation systems. 
6. An AI-based real estate platform uses a single-layer neural network to estimate house price 
scores based on multiple input features: ğ‘¥1: House size (in 100 sq. ft), ğ‘¥2: Distance from 
the city center, ğ‘¥3: Age of the house. The input vector, weight matrix, and bias vector for 
the hidden linear layer (2 neurons) are given by: 
ğ‘‹ = [2 â€Š â€Š1 â€Š â€Š3], ğ‘Š = [0.3 âˆ’0.2 âˆ’0.1
0.4 âˆ’0.3 âˆ’0.2] , ğ‘ = [0.2
0.3] 
The final output layer (1 neuron) is defined as: 
ğ‘Š2 = [0.2 â€Š â€Š0.3], ğ‘2 = 0.2 
Apply the linear transformation to compute the hidden layer output and also determine the 
final output of the neural network and illustrate the linear layer neural network model using 
a neat, labeled diagram from the above data. 
 
 
 


--- END OF HA-1.pdf ---



--- START OF Home Assignment-2.pdf ---

 
Home Assignemnt-2 
1. A map uses a linear transformation represented by ğ´ = [3 0
0 âˆ’2] to stretch regions 
vertically. If the original area is 1 2 square units, calculate the area of the transformed 
region. 
2. State the implication of two feature vectors having a zero-dot product. 
3. Compute the determinant of the matrix ğµif the eigenvalues of a square matrix are 
15,0,3. 
4. Determine the rank of the feature set and identify the redundant feature(s) for the 
vectors ğ‘¥1 = (1,1,1), â€Šğ‘¥2 = (2,2,2), â€Šğ‘¥3 = (0,1,1). 
5. Compute the singular values of the matrix ğ´ = [1 2 3
2 3 âˆ’2]. 
6. Explain why high-dimensional data often contains redundant information. 
7. An industrial plant monitors machine health using multiple sensors. At a given 
machine, the following features are collected: 
ğ‘“1: Motor vibration amplitude 
ğ‘“2: Root mean square (RMS) vibration 
ğ‘“3: Machine temperature 
ğ‘“4: Thermal stress index 
ğ‘“5: Power consumption 
ğ‘“6: Electrical load factor 
The plant wants to minimize data processing cost by eliminating highly correlated 
features. Analyze which features are likely redundant and identify the essential features 
to retain. 
8. A grayscale image is represented by ğ´ = [3 2
1 0]. Obtain the SVD of ğ´ and determine 
the rank of ğ´, also construct the rank-1 approximation using the largest singular value. 
9. Consider the following matrices:  (a)  ğ´ = [1 1 0
0 1 1] (b) ğ´ = [
âˆ’1 âˆ’2
1 2
1 2
]  For each 
matrix ğ´, (i) compute its Singular Value Decomposition (SVD) and express it in the 
form ğ´ = ğ‘ˆâ€‰Î£â€‰ğ‘‰ğ‘‡.(ii) Clearly specify the dimensions of the matrices ğ‘ˆ, Î£, and 
ğ‘‰.(iii)Identify the singular values and determine the rank of each matrix.(iv) Comment 
on how the shape of matrix ğ´(rectangular vs square) affects the structure of its SVD. 
10. A grayscale image is represented by the following pixel -intensity matrix ğµ = [
5 4
3 2
1 0
] 
Using the above matrix, (i) compute the Singular Value Decomposition (SVD) of the 
matrix ğµ, i.e., express ğµ = ğ‘ˆÎ£ğ‘‰ğ‘‡,(ii) determine the rank of the matrix ğµ, (iii) construct 
a rank-1 approximation of the matrix ğµ, also explain the significance of Singular Value 

Decomposition in image compression, emphasizing how dimensionality reduction 
helps preserve the most important visual information while reducing data size. 
 
 
 
 


--- END OF Home Assignment-2.pdf ---

